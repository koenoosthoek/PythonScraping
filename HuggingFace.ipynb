{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP with Hugging Face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to import Hugging Face:\n",
    "\n",
    "# Note that you need to have Tensorflow 2.0 and/or PyTorch before!\n",
    "\n",
    "# install by typing the following to your terminal: \n",
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pipeline Function:\n",
    "\n",
    "The Pipeline Function allows you to easily perform a wide range of text analyses.\n",
    "\n",
    "It covers the whole (basic) workflow, including important pre- and postprocessing steps, as well as the modelling part (This is where the name comes from).\n",
    "\n",
    "By default, pipeline employs pretrained models from it's API, which work pretty well for general (english-language) purposes. However, for a specific project you may want to train/finetune a model yourself. Training your own model is a bit more advanced, but you can find very helpful tutorials here: \n",
    "- \n",
    "- \n",
    "You also find a wide range of pretrained models shared on https://huggingface.co/models, covering many tasks and different languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pipeline:\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12452/2829773413.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# define task:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msentiment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sentiment-analysis\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m# classify:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m sentiment([\"I've been waiting for HuggingFace all my life.\",\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\pipelines\\__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, model_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    507\u001b[0m                 \u001b[0mtokenizer_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m             tokenizer = AutoTokenizer.from_pretrained(\n\u001b[0m\u001b[0;32m    510\u001b[0m                 \u001b[0mtokenizer_identifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_fast\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_fast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_from_pipeline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtokenizer_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m             )\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    503\u001b[0m             \u001b[0mtokenizer_class_py\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTOKENIZER_MAPPING\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0muse_fast\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 505\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    506\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1649\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1650\u001b[0m             \u001b[1;31m# At this point pretrained_model_name_or_path is either a directory or a model identifier name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1651\u001b[1;33m             fast_tokenizer_file = get_fast_tokenizer_file(\n\u001b[0m\u001b[0;32m   1652\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1653\u001b[0m                 \u001b[0mrevision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mget_fast_tokenizer_file\u001b[1;34m(path_or_repo, revision, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   3465\u001b[0m     \"\"\"\n\u001b[0;32m   3466\u001b[0m     \u001b[1;31m# Inspect all files from the repo/folder.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3467\u001b[1;33m     all_files = get_list_of_files(\n\u001b[0m\u001b[0;32m   3468\u001b[0m         \u001b[0mpath_or_repo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3469\u001b[0m     )\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mget_list_of_files\u001b[1;34m(path_or_repo, revision, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   1816\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1817\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1818\u001b[1;33m     model_info = HfApi(endpoint=HUGGINGFACE_CO_RESOLVE_ENDPOINT).model_info(\n\u001b[0m\u001b[0;32m   1819\u001b[0m         \u001b[0mpath_or_repo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1820\u001b[0m     )\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\hf_api.py\u001b[0m in \u001b[0;36mmodel_info\u001b[1;34m(self, repo_id, revision, token, timeout)\u001b[0m\n\u001b[0;32m    582\u001b[0m             \u001b[1;33m{\u001b[0m\u001b[1;34m\"authorization\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Bearer {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m         )\n\u001b[1;32m--> 584\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    585\u001b[0m         \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \"\"\"\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    540\u001b[0m         }\n\u001b[0;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                     \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[1;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[1;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1008\u001b[0m         \u001b[1;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sock\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1010\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1011\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    414\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_default_certs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m         self.sock = ssl_wrap_socket(\n\u001b[0m\u001b[0;32m    417\u001b[0m             \u001b[0msock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m             \u001b[0mkeyfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\util\\ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msend_sni\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m         ssl_sock = _ssl_wrap_socket_impl(\n\u001b[0m\u001b[0;32m    450\u001b[0m             \u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtls_in_tls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m         )\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\util\\ssl_.py\u001b[0m in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;31m# SSLSocket class handles server_hostname encoding before it calls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[1;31m# ctx._wrap_socket()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m         return self.sslsocket_class._create(\n\u001b[0m\u001b[0;32m    501\u001b[0m             \u001b[0msock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m             \u001b[0mserver_side\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mserver_side\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ssl.py\u001b[0m in \u001b[0;36m_create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1038\u001b[0m                         \u001b[1;31m# non-blocking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1307\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0.0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1310\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define task:\n",
    "sentiment = pipeline(\"sentiment-analysis\",)\n",
    "# classify:\n",
    "sentiment([\"I've been waiting for HuggingFace all my life.\",\n",
    "            \"My life was so hard without HuggingFace\"]\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statement Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to roberta-large-mnli (https://huggingface.co/roberta-large-mnli)\n",
      "Downloading: 100%|██████████| 688/688 [00:00<00:00, 684kB/s]\n",
      "Downloading: 100%|██████████| 1.33G/1.33G [03:37<00:00, 6.57MB/s]\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at roberta-large-mnli.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "Downloading: 100%|██████████| 878k/878k [00:00<00:00, 1.50MB/s]\n",
      "Downloading: 100%|██████████| 446k/446k [00:00<00:00, 1.09MB/s]\n",
      "Downloading: 100%|██████████| 1.29M/1.29M [00:00<00:00, 1.89MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'After this course, you will know how to use the transformers library',\n",
       " 'labels': ['education', 'advertisement', 'politics'],\n",
       " 'scores': [0.862166166305542, 0.11096455901861191, 0.026869328692555428]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# again, i'ts only one line to define the task:\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "classifier(\n",
    "    \"After this course, you will know how to use the transformers library\",\n",
    "    candidate_labels = [\"advertisement\", \"politics\", \"education\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)\n",
      "Downloading: 100%|██████████| 998/998 [00:00<00:00, 250kB/s]\n",
      "Downloading: 100%|██████████| 1.24G/1.24G [03:23<00:00, 6.57MB/s]\n",
      "Some layers from the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing TFBertForTokenClassification: ['dropout_147']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForTokenClassification were initialized from the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n",
      "Downloading: 100%|██████████| 60.0/60.0 [00:00<00:00, 60.0kB/s]\n",
      "Downloading: 100%|██████████| 208k/208k [00:00<00:00, 983kB/s] \n",
      "C:\\Users\\milan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\pipelines\\token_classification.py:128: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9982297,\n",
       "  'word': 'Milan',\n",
       "  'start': 4,\n",
       "  'end': 9},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.92548555,\n",
       "  'word': 'Hertie School',\n",
       "  'start': 52,\n",
       "  'end': 65},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9911318,\n",
       "  'word': 'Berlin',\n",
       "  'start': 69,\n",
       "  'end': 75}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities = True)\n",
    "\n",
    "ner(\"I'm Milan, and I'm a second year MIA student at the Hertie School in Berlin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text generation (with some randomness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 (https://huggingface.co/gpt2)\n",
      "Downloading: 100%|██████████| 665/665 [00:00<00:00, 331kB/s]\n",
      "Downloading: 100%|██████████| 475M/475M [01:14<00:00, 6.68MB/s]\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Downloading: 100%|██████████| 0.99M/0.99M [00:00<00:00, 1.10MB/s]\n",
      "Downloading: 100%|██████████| 446k/446k [00:00<00:00, 905kB/s]\n",
      "Downloading: 100%|██████████| 1.29M/1.29M [00:01<00:00, 1.25MB/s]\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': 'Hello, how are you today?\\n\\nI was at work, sitting on the piano. I think that was the first time that I actually stopped to be so honest with you about my job, because I knew exactly how much you wanted me to'}],\n",
       " [{'generated_text': 'The most important thing in life is to create a better life. Everything you do can be changed, but not everything that goes into your job. You can make different decisions, but at least you know what you\\'re doing.\\n\\nWe believe \"'}]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model = \"gpt2\")\n",
    "generator([ \"Hello, how are you today?\",\n",
    "            \"The most important thing in life is\"],\n",
    "            max_length = 30,\n",
    "            num_return_sequences = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad)\n",
      "Some layers from the model checkpoint at distilbert-base-cased-distilled-squad were not used when initializing TFDistilBertForQuestionAnswering: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased-distilled-squad and are newly initialized: ['dropout_59']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.774193286895752, 'start': 25, 'end': 38, 'answer': 'Hertie School'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answerer = pipeline(\"question-answering\")\n",
    "\n",
    "answerer(\n",
    "    question = \"Where do I study?\",\n",
    "    context = \"I'm a MIA student at the Hertie School in Berlin, Germany\" \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unmasking/word Guessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base (https://huggingface.co/distilroberta-base)\n",
      "All model checkpoint layers were used when initializing TFRobertaForMaskedLM.\n",
      "\n",
      "All the layers of TFRobertaForMaskedLM were initialized from the model checkpoint at distilroberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'My favourite artist of all time is Madonna!',\n",
       "  'score': 0.03797246515750885,\n",
       "  'token': 23064,\n",
       "  'token_str': ' Madonna'},\n",
       " {'sequence': 'My favourite artist of all time is Elvis!',\n",
       "  'score': 0.020540988072752953,\n",
       "  'token': 20131,\n",
       "  'token_str': ' Elvis'},\n",
       " {'sequence': 'My favourite artist of all time is Raphael!',\n",
       "  'score': 0.019978316500782967,\n",
       "  'token': 29719,\n",
       "  'token_str': ' Raphael'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\")\n",
    "\n",
    "unmasker(\"My favourite artist of all time is <mask>!\", top_k = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-small (https://huggingface.co/t5-small)\n",
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5319 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \"john avlon: we have to rebuild our country, and we can't do it . he says we have a lot of good people, but we've got to do it fast, a tv show . we're going to be in a different direction, he writes, and he's going to win . but he doesn't want to be a president, but it's not a bad thing, bvv shows .\"}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "trump_speech = \"\"\"\n",
    "Thank you so much. That's so nice. Isn't he a great guy. He doesn't get a fair press; he doesn't get it. It's just not fair. And I have to tell you I'm here, and very strongly here, because I have great respect for Steve King and have great respect likewise for Citizens United, David and everybody, and tremendous resect for the Tea Party. Also, also the people of Iowa. They have something in common. Hard-working people. They want to work, they want to make the country great. I love the people of Iowa. So that's the way it is. Very simple.\n",
    "With that said, our country is really headed in the wrong direction with a president who is doing an absolutely terrible job. The world is collapsing around us, and many of the problems we've caused. Our president is either grossly incompetent, a word that more and more people are using, and I think I was the first to use it, or he has a completely different agenda than you want to know about, which could be possible. In any event, Washington is broken, and our country is in serious trouble and total disarray. Very simple. Politicians are all talk, no action. They are all talk and no action. And it's constant; it never ends.\n",
    "And I'm a conservative, actually very conservative, and I'm a Republican. And I'm very disappointed by our Republican politicians. Because they let the president get away with absolute murder. You see always, oh we're going to do this, we're going to--. Nothing ever happens; nothing ever happens.\n",
    "You look at Obamacare. A total catastrophe and by the way it really kicks in in '16 and it is going to be a disaster. People are closing up shops. Doctors are quitting the business. I have a friend of mine who's a doctor, a very good doctor, a very successful guy. He said, I have more accountants than I have patients. And he needs because it is so complicated and so terrible and he's never had that before and he's going to close up his business. And he was very successful guy. But it's happening more and more.\n",
    "Look at Obamacare with a $5 billion website. I have many websites, many, many websites. They're all over the place. But for $10, okay?\n",
    "Now everything about Obamacare was a lie. It was a filthy lie. And when you think about it, lies, I mean are they prosecuted? Does anyone do anything? And what are the Republican politicians doing about it? He lied about the doctor, he lied about every aspect. You can keep your plan. And you've all heard that hundreds of times. That's like the real estate location, location. I don't even say it anymore because everybody goes location, location. But you have heard this about Obamacare.\n",
    "And it's disgraceful. It's a big, fat, horrible lie. Your deductibles are going through the roof. You're not going to get--unless you're hit by an army tank, you're not going to get coverage. And people that had plans that they loved, that they really loved, don't have those plans anymore. So it's a real, real disaster. And somebody has to repeal and replace Obamacare. And they have to do it fast and not just talk about it.\n",
    "Now, we have to build a fence. And it's got to be a beauty. Who can build better than Trump? I build; it's what I do. I build; I build nice fences, but I build great buildings. Fences are easy, believe me. I saw the other day on television people just walking across the border. They're walking. The military is standing there holding guns and people are just walking right in front, coming into our country. It is so terrible. It is so unfair. It is so incompetent. And we don't have the best coming in. We have people that are criminals, we have people that are crooks. You can certainly have terrorists. You can certainly have Islamic terrorists. You can have anything coming across the border. We don't do anything about it. So I would say that if I run and if I win, I would certainly start by building a very, very powerful border.\n",
    "Again, the politicians talk about it and they do nothing about it. Benghazi. Oh, Benghazi, Benghazi. Everything is Benghazi. What happens? Nothing.\n",
    "IRS, e-mails. I get sued all the time, okay. I run a big business. You know I've always said it's very, very hard for a person who is very successful. I have done so many deals. Almost all of them have been tremendously successful. You'll see that when I file my statements. I mean you will see; you will be very proud of me, okay. But I've always said, and I said it strongly, it's very hard for somebody that does tremendous numbers of deals to run for politics, run for political office, any office, let alone president. Because you've done so much; you've beaten so many people; you've created so many-- Look, Obama, what did he do? No deal. He never did a deal. He did one deal. A house. And if you did that house you'd be in jail right now, okay. He got away with murder. But I can tell you, e-mails. IRS, the e-mails, thousands of them, they were lost; they were lost. If you were in my world you would know that e-mails can't be lost; they can't be lost. So why aren't our politicians finding out where those e-mails are?\n",
    "They talk about executive orders and they talk about immigration and they talk about oh well we have to stop the border; that's the end of it. Believe me if I did something you would have a border that would be great. But they talk about it. And then you have a president that does an executive order. Nobody even heard of an executive order. He does it to let people come in and nobody does anything about executive orders. Why didn't they go to court and ask for declaratory judgment--which is something that when you know somebody is going to go after you and when it's in writing, and he's been saying it for a long time; he said, I'm going to approve this and that--why didn't our Republicans go in and get a declaratory judgment from the courts because you could have started the process six months earlier. Instead they have a weak lawsuit, that probably the time it's finished, I know so much about this, six, seven, eight years from now everyone's going to forget about it. We'll be into a different mode, and our country will be further destroyed. So we have to do something.\n",
    "Jobs. China. I mean I've made so much money fighting against the Chinese. One of the best deals I ever did was against the Chinese, and they respect me for it. And I know them. And they say, we can't believe what we're getting away with. We can't believe how stupid your leaders are. They tell me that. Now they don't know I'm going to go and make a speech about it, but why not. But they tell me that.\n",
    "And by the way, especially for the folks here that sell so much--goods, I mean the goods you sell are incredible; I don't know if you've been watching what's happening with the devaluations of so many countries. The Euro, China is going crazy with the devaluation. I never thought that they'd have the guts to do what they're doing. They are devaluing down to nothing. And what it's going to do is make it impossible for you to sell your product; it's going make it impossible for you to compete. And they're getting away with it. And they wouldn't have even done it two years ago, but now they feel we're so weak and we have so many different problems all over the world that they can do it. But you watch this devaluation of all of it. I mean the Euro, China, Mexico; everybody is devaluing. And when you hear the dollar is getting stronger, it sounds good. You know it's one of those things, sounds good. Be very careful. Be very careful. Because we're just going to lose more and more business to these foreign countries that really know what they are doing. They have it set. Don't forget another thing. China became the number one economic power a year ago. That was unthinkable; to think that that was going to happen. It was absolutely unthinkable. So it happened and it's very, very sad.\n",
    "Now we spent $2 trillion in Iraq. We got nothing. They don't even respect us; they don't even care about us. Until they started getting their ass kicked, and call, oh please come back and help us. We want you out; then all of the sudden a new group forms, ISIS. By the way, you know how they formed. They took the oil. And for those of you that know and love Donald Trump--there are some of you--have I been saying for four years, keep the oil. So now ISIS has the oil. And the stuff that ISIS doesn't have, Iran is going to take. So we get nothing. We have $2 trillion and we have thousands of lives lost, thousands, and we have, what do we have. I mean I walk down the streets of New York and I see so many wounded warriors, incredible people. And we have to help those people, we have to help our vets, we have to help our military, we have to build our military. But, and we have to do it fast; we have to do it fast. We have incompetent people. They put people in charge that have no clue what they're doing. It needs money.\n",
    "We have to make our country rich again so we do that, so we can save Social Security. 'Cause I'm not a cutter; I'll probably be the only Republican that does not want to cut Social Security. I'm not a cutter of Social Security; I want to make the country rich so that Social Security can be afforded, and Medicare and Medicaid. Get rid of the waste, get rid of the fraud, but you deserve your Social Security; you've been paying your Security. And like, I like Congressman Ryan, I like a lot of the people that are talking about you know cutting Social Security, and by the way the Democrats are eating your lunch on this issue. It's an issue that you're not going to win; you've got to make the country rich again and strong again so that you can afford it, and so you can afford military, and all of the other things.\n",
    "Now, we have a game changer now, and the game changer is nuclear weapons. We really do have to get strong, and we have to get strong fast. We can't let Iran get a nuclear weapon. We can't do it. Can't do it. We cannot let that happen. You know in the old days, I would have said 100 years ago, 50 years ago, 30 years ago, pull out and let them fight each other.\n",
    "Here we are in Syria. We're fighting people that want to overturn Syria. Think of this. We're fighting ISIS, but ISIS wants to overturn the government. Maybe you let them fight for a little while and then you take out the one that remains, okay. But think of it; think of it. ISIS is fighting them and we are bombing the hell out of them, but we want Syria to fall. There are so many things; there are so many things.\n",
    "But the one game changer that we have to be careful with, that we never really had to think about too much before, other than a cetain number of years ago, is the nuclear. Nuclear today, it's not like soldiers in uniforms shooting rifles. You can take out the East Coast of this country, you can take out large sections of the Midwest, you can take out things that were unthinkable. The power. And we have to be in a position where that never, ever, ever, every happens. We've never had this before.\n",
    "We have a situation in Afghanistan; we're spending tremendous amounts of money there. We're trying to do the right thing. We have leadership, again--no leadership respects us. You know, leadership of other places never respect stupid people, okay, that's one thing you're going to find. The same thing is happening there. And I never knew that Afghanistan until a year ago or so, Afghanistan has tremendous wealth in minerals, different, not the oil, but minerals. And we're fighting here, and on the other side of the mountain China is taking out all the minerals. They're taking it out. Trillion of dollars and millions of dollars of minerals. So we're fighting here and they are taking it out, looking at us and saying thank you very much sucker. It's really, really crazy.\n",
    "So we have to rebuild quickly our infrastructure of this country. If we don't-- The other day in Ohio a bridge collapsed. Bridges are collapsing all over the country. The reports on bridges and the like are unbelievable, what's happening with our infrastructure.\n",
    "I go to Saudi Arabia, I go to Dubai; I am doing big jobs in Dubai. I go to various different places. I go to China. They are building a bridge on every corner. They have bridges that make the George Washington Bridge like small time stuff. They're building the most incredible things you have ever seen. They are building airports in Qatar--which they like to say \"cutter\" but I've always said \"qatar\" so I'll keep it \"qatar\" what the hell. But they're building, they're building an airport and have just completed an airport the likes of which you have never seen, in Dubai an airport the likes of which you have never seen. And then I come back to LaGuardia where the runways have potholes. The place is falling apart. You go into the main terminal and they have a terraza floor that's so old it's falling apart. And they have a hole in it, and they replace it with asphalt. So you have a white terraza floor and they put asphalt all over the place. This is inside, not outside. And I just left Dubai where they have the most incredible thing you've ever seen. In fact my pilot said oh Mr. Trump this is such an honor. I said it's not an honor; they're just smart. But you look at LAX, and you look at Kennedy Airport, and you look at our airports generally, you look at our roadways where they're crumbling.\n",
    "You look at all of the things that are bad-- I'll give you an example. And this isn't part of what I was going to say, but I ride down the highways and somebody makes those guard rails. You know the guard rails. The ones that sort of go like this [demonstrates with hand] that are always bent, rusted and horrible. Did you ever see more than like 20 feet which isn't corroded, or bent or the heat, if it gets too hot, it just crushes. Now they've been selling this thing for 25 years. Why doesn't someone stop them and get something that works. Because they don't know; they don't know what's happening. Somebody made a lot of money on that. They don't know what is happening.\n",
    "So we have to make our country great again. We have to rebuild our country. And we have a long way to go. We are just in such serious trouble because we owe so much money. Now we owe it to the Chinese, a lot of it. We owe it to other countries. They're the ones that hold the debt. And then we give them money. We have countries that we owe money to and yet we're giving them subsidies. I just ordered thousands of television sets and between LG and Samsung and I mean you know-- No American company comes to see and comes to bid. It's South Korea, and whenever they have a problem we send the battleships, we send the destroyers, we send our airplanes, we're going to protect them. What are we doing; why aren't they paying us? Why aren't they paying us; what are we doing? I order thousands and thousands of sets all the time, for some reason it's South Korea. You know whether it's China, South Korea, but in this case televisions, South Korea. Why aren't they doing something to justify what is going on?\n",
    "Now, we have a very important election coming up. We have a presidential election coming up. And we have some good people. Nobody like Trump of course, but these are minor details. We have some good people.\n",
    "It' can't be Mitt because Mitt ran and failed. He failed. I mean I liked him. Look--like him, dislike him--the 47% statement that he made, that's not going away. The Romneycare from Massachusetts, that's not going away. What do you think they're going to say oh we won't bring that up this time. It's not fair because it was a long t-- That doesn't work. But more importantly, he choked. Something happened to him in the last month. He had that election won. And let me tell you something. That election, sort of like a dealmaker that can't close the deal. I know many of those guys; they get it up to the one-yard line, they go ah, ah I can't close it. Or a golfer that can't sink the three-footer to win the tournament. And there are many of them. Most people are like that; I mean most people are like that. You can't give somebody another chance, 'cause actually I think this election is tougher to win than beating a failed president. I really do. I think beating Obama would have been a much easier one than the one that's coming up, which is sad to say but true. So you can't have Romney. He choked.\n",
    "You can't have Bush. The last thing we need is another Bush. Now I made that statement very strongly, and now every one says the last thing-- You know they copied it. I'll be accused of copying the statement; that's the bad thing. But I said it. I was the one that said it first, and I mean it. The last thing we need is another Bush. Now, he's totally in favor of Common Core; that's a disaster, that's bad, it should be local and all of that. But he's totally in favor of Common Core. He's very, very weak on immigration. Don't forget--remember his statement--they come for love. I say, what? Come for love? You've got these people coming, half of them are criminals. They're coming for love? They're coming for a lot of other reasons, and it's not love. And when he runs, you got to remember his brother really gave us Obama. I was never a big fan, but his brother gave us Obama. 'Cause Abraham Lincoln coming home back from the dead could not have won the election because it was going so badly and the economy was just absolutely in shambles that last couple of months. And then he appointed Justice Roberts. And Jeb wanted Justice Roberts. And Justice Roberts basically approved Obamacare in a shocking decision that nobody believes. So you can't have Jeb Bush. And he's going to lose aside from that; he's not going to win. So Mitt and--you just can't have those two. That's it. That's it. It's so simple.\n",
    "So just in summing up and I just wrote a few of these little points down because it's very important. And I watch these teleprompters, and by the way I think any president candidate that runs should not be allowed to use a teleprompter, because we got one that uses teleprompters. And people say, oh he is so quick on his feet. He is reading it. I mean give me a break. Everything is read. You don't really test the mettle of a man or a woman unless they can get up on stage and talk. And that's what we ended up getting--the king of teleprompters. But, so when I look at these things here I say you know what, it's so much easier, it would be so nice, just bah, pa, bah, pa, bah, bing, bing, bing. No problems, get off stage, everybody falls asleep and that's the end of that. But we have to do something about these teleprompters.\n",
    "But in looking at these situations-- I built an incredible company. And you'll see that. An incredible company; a wonderful company. I employ thousands of people and I love doing what I'm doing. And in a certain way, I wish I weren't doing this, but our country is in such trouble and would be so easy to fix. We have such great potential.\n",
    "So if I run for president and if I win, I would totally succeed in:\n",
    "creating jobs;\n",
    "defeating ISIS and stopping the Islamic terrorists--and you have to do that;\n",
    "reducing the budget deficit--so important, have to do it;\n",
    "securing our Southern border--and I mean seriously securing it;\n",
    "stopping nuclear weapons in Iran and elsewhere;\n",
    "saving Social Security, Medicare and Medicaid, without cutting it down to the bone because it's not fair to people that have been paying for their whole lives and other people and it's not fair to future people coming up, and we can do it;\n",
    "repealing Obamacare and replacing it with something far better for the people, and far less expensive, both for the people and for the country. And believe me there are plans that are so much better for everybody. And everybody can be covered. I'm not saying leave 50-percent of the people out. Everybody can be c-- This plan is just a basic disaster. Nobody knows. As bad as the website was, this is how bad the plan itself is;\n",
    "fixing our country's infrastructure, our bridges, our schools, our highways, our airports. And that, I can tell you, nobody is close to Trump. I just got the best hotel in North America. I'm building, which is sort of interesting, think of this one. I'm building one of the great hotels of the world on Pennsylvania Avenue, right opposite the White House, between the White House and Congress. Right on Penn.-- The Old Post Office site, and I got it from Obama. Do you believe that? And everybody wanted it. So, I can't believe it myself. But it's going to be fantastic;\n",
    "and so many other things.\n",
    "I know what needs to be done to make America great again. We can make this country great again. The potential is enormous. And I am serious thinking of running for president because I can do the job.\n",
    "Thank you all very much. Thank you. Thank you. Thank you. Thank you. Thank you very much, everybody. Thank you. Beautiful. Thank you.\n",
    "\"\"\"\n",
    "summarizer(trump_speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation\n",
    "\n",
    "#### DOESN'T WORK!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
      "\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-de.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12452/1844003990.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtranslator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"translation\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Helsinki-NLP/opus-mt-en-de\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"This is a beautiful day.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\pipelines\\__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, model_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    507\u001b[0m                 \u001b[0mtokenizer_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m             tokenizer = AutoTokenizer.from_pretrained(\n\u001b[0m\u001b[0;32m    510\u001b[0m                 \u001b[0mtokenizer_identifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_fast\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_fast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_from_pipeline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtokenizer_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m             )\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    508\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m                     raise ValueError(\n\u001b[0m\u001b[0;32m    511\u001b[0m                         \u001b[1;34m\"This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m                         \u001b[1;34m\"in order to use this tokenizer.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer."
     ]
    }
   ],
   "source": [
    "translator = pipeline(\"translation\", model = \"Helsinki-NLP/opus-mt-en-de\")\n",
    "\n",
    "translator(\"This is a beautiful day.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Cool Functions:\n",
    "\n",
    "Transformers has many more functions to offer, including some to process audio or visual data.\n",
    "\n",
    "Have a look on https://huggingface.co/models to find the one that fits your purpose!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ff17ad368032c9d1e6c86530981ccaeb979061e539b84c70f30ac8107311ba4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
